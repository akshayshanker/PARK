\documentclass[]{article}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{graphicx}
% Redefine \includegraphics so that, unless explicit options are
% given, the image width will not exceed the width of the page.
% Images get their normal width if they fit onto the page, but
% are scaled down if they would overflow the margins.
\makeatletter
\def\ScaleIfNeeded{%
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother
\let\Oldincludegraphics\includegraphics
{%
 \catcode`\@=11\relax%
 \gdef\includegraphics{\@ifnextchar[{\Oldincludegraphics}{\Oldincludegraphics[width=\ScaleIfNeeded]}}%
}%
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={Christopher D. Carroll and Nathan M. Palmer},
            pdftitle={The Heterogeneous-Agent Computational toolKit: An Extensible Framework for Solving and Estimating Heterogeneous-Agent Models},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{5}
\usepackage{fancyhdr}
\pagestyle{fancy}

\title{The Heterogeneous-Agent Computational toolKit: An Extensible Framework
for Solving and Estimating Heterogeneous-Agent Models}
\author{Christopher D. Carroll and Nathan M. Palmer}
\date{June 20, 2015}



\begin{document}
\maketitle

\begin{abstract}
We present a modular and extensible toolkit for solving and estimating
heterogeneous-agent partial- and general-equilibrium models.
Heterogeneous-agent models have grown increasingly valuable for both
policy and research purposes, but code to solve such models can be
difficult to penetrate for researchers new to the topic. As a result it
may take years of human capital development for researchers to become
proficient in these methods and contribute to the literature. The goal
of the HACK toolkit is to ease this burden by providing a simple and
easily extensible framework in which a few common models are solved, and
clear documentation, testing, and estimation examples provide guidance
for new researchers to develop their own work in a robust and replicable
manner. Using two examples, we outline key elements of the toolkit which
ease the burden of learning, using, and contributing to the codebase.
This includes a simple API for model solution and an API for estimation
via simulation, as well as methods for bundling working code with
interactive documentation. The foundational solution method we employ is
Carroll (2012), ``Solution Methods for Microeconomic Dynamic Stochastic
Optimization Problems,'' written in a modular Python framework. We
briefly discuss a number of extensions as well as tertiary projects
implied by this effort.
\end{abstract}

\centerline{JEL codes E21, C61, E63}

\section{Introduction}\label{introduction}

The Heterogeneous-Agent Computation toolKit (HACK) is a modular
programming framework for solving and estimating macroeconomic and
macro-financial models in which economic agents can be heterogeneous in
a large number of ways. Models with extensive heterogeneity among agents
can be extremely useful for policy work. For example, Carroll (2012b,
2014a, 2014b) and Carroll et al. (2014) demonstrate how aggregate
consumption and output can be heavily influenced by heterogeneity.
Geanakoplos' (2009) outlines how heterogeneity drives the leverage
cycle, and Geanakoplos et al. (2012) applies these insights to
large-scale model of the housing and mortgage markets. However the most
commonly published macroeconomic and macro-finance models have very
limited heterogeneity or none at all (this includes the large class of
representative agent models), in large part because these are the only
models which can be easily solved with existing toolkits.\footnote{Dynare
  is the most popular toolkit for representative-agent models. For more
  details see Adjemian et al. (2011).} In contrast, models with
extensive heterogeneity among agents have no central toolkit and must be
solved in a bespoke way. This requires a significant investment of time
and human capital before a researcher can produce publishable or usable
work. This results in needless code duplication, increasing the chance
for error and wasting valuable research time.

The HACK project addresses these concerns by providing a set of
well-documented code modules which can be composed together to solve a
range of heterogeneous-agent models. Methodological advances in the
computational literature allow many types of models to be solved using
similar approaches -- the HACK toolkit simply brings these together in
one place. The key is identifying methodologies which are both
``modular'' (in a sense to be described below) as well as robust to
model misspecification. These include both solution methods as well as
estimation methods.
\href{We\%20outline\%20a\%20number\%20of\%20specific\%20a}{}

In addition to these methodological advances, the HACK project adopts a
few modern practices from the field of software development to ease the
burden on code review, code sharing, and programming collaboration for
researchers dealing in computational methods. Researchers who must
review the scientific and technical code written by others are keenly
aware that the time required to review and understand another's code can
easily dwarf the time required to simply re-write the code from scratch
(conditional on understanding the underlying concepts). This can be
particularly important when multiple researchers may need to work on
parts of the same codebase, either across time or distance. This problem
is not confined to scientific computing alone. Fortunately the software
development community, and particularly the open-source community, has
spent decades perfecting tools for programmers to quickly consume and
understand code written by others, verify that it is correct, and
proceed to contribute back to a large and diverse codebase without fear
of introducing bugs. The tools used by these professional developers
include formal code documentation, unit testing structures, modern
versioning systems for automatically tracking changes to code and
content, and low-cost systems of communicating ideas, such as
interactive programming notebooks which combine formatted mathematics
with executable code and descriptive content. These tools often operate
in concert with one another, forming a powerful infrastructure which can
greatly accelerate project development for both individuals and
collaborative teams. These technical tools are not new -- the HACK
project simply aims to apply the best of them to scientific code in a
structured way to increase researcher productivity, particularly when
interacting with other researchers' code.

\href{Insert\%20literature\%20review\%20of\%20software\%20development\%20practices\%20as\%20well\%20as\%20the\%20baseline\%20models\%20employed.}{}

The project presented here is not an attempt to create new methodology
either on the software development front or the research front (although
we expect new methodological contributions to emerge from the effort).
Rather the HACK project brings together many well-understood and proven
methodologies to bear in an easily used and extended toolkit. The rest
of this paper will first outline the useful concepts adopted from
software development, with examples of each. If the reader is a
practiced and experienced programmer, he or she may wish to skip
directly to Section 3 to see how these ideas are applied to the specific
consumer problem employed. The sections are organized as follows:
Section 2 outlines key tools from professional software development.
Section 3 discusses the theoretical problem which provides the framework
for the HACK project and outline of the first example model under
development in the HACK framework. Section 4 outlines key next steps and
concludes.

\section{Tools from Software
Development}\label{tools-from-software-development}

Before progressing to a specific example, this section provides little
background about the specific software tools HACK leverages. The breadth
and history of software development is extensive, and review of it is
beyond the scope of this document. One of the most striking practices to
emerge from this history, however, is open-source software development
-- the decentralized collaboration of many independent programmers on a
single project, often with little or no immediate monetary reward. Aside
from fascinating theoretical questions about incentive structures, the
open-source movement has produced a wide array of excellent utilities
which make decentralized code development robust and efficient. These
utilities are closely intertwined with those from the traditional
software development world; this section briefly overviews a number
tools from both. The following section outlines how the solution to a
basic economic problem can be developed using these utilities.

There are a number of resources which delve deeply into the topics
discussed in this section. For an excellent review of many of these
topics from an economists perspective, see the unparalleled set of
lectures by Sargent and Stachurski (2015)\href{REF}{}, which can be
accessed at the time of this writing at the
\href{http://www.quant-econ.net}{Quant-Econ webpage}. The Python
programming language is the primary language used for development of
HACK; many resources related to this language can be found on the
primary \href{https://www.python.org/}{Python webpage}. Quant-Econ is an
excellent introduction to Python for economists, as is Sheppard
(2014)\href{REF}{}. Aruoba and FernÃ¡ndez-Villaverde (2014) provide a
nice comparison of many programming languages for computational
economics, including Python.

\subsection{An Aside on Speed}\label{an-aside-on-speed}

Python began as an interpreted scripting language and originally was
many hundreds or thousands of times slower than compiled languages such
as C++. Over time as the scientific community has adopted Python more
extensively, a number of projects have emerged which allow Python to be
compiled many different ways. At this time, there are a number of
options for accelerating Python code. This is reflected in Table 1 in
Aruoba and FernÃ¡ndez-Villaverde (2014), which compares a number of
programming languages against C++ for a loop-intensive task. When sorted
by relative time against the fastest C++ implementation, Python occupies
the fastest two spots which are not other C++ or FORTRAN.\footnote{The
  first five spots in the relative ranking are occupied by different
  compiler implementations of C++ and FORTRAN. The 6th and 7th ranks are
  occupied by two of the most popular Python compilers, Cython and
  Numba, respectively, which are 1.41 and 1.65 times slower than the
  fastest C++ implementation. Notably, two C++ implementations are 1.38
  times slower than the fastest, and one of the two FORTRAN
  implementations is 1.30 times slower than the fastest C++
  implementation. That is, the fastest Python implementation is only
  about 3\% slower than two of the three C++ implementations.} This is
not a definitive illustration of the speed capabilities of Python, as
there are many caveats which must be considered in the problem setup and
execution (as noted by the authors themselves). However it does serve to
illustrate that Python is capable of very high speeds when compiled.
Furthermore, even aside from compilation, when Python is vectorized
using the major numerical libraries, NumPy and SciPy, all vectorized
calculations are executed in optimized, compiled C and FORTRAN.

The one caveat to statements about Python speed is in order for complex
code structures. Object-oriented programming structures in Python can
prove difficult to compile easily. Extensive computations on class-based
objects can impose significant speed penalties. There are a number of
ways around this. The simplest is to write code which does not require
class structure -- simple functional libraries. This is the approach
HACK takes. This allows individual functions to be accelerated via
vectorization or compilation, maximizing speed potential. If a class
structure cannot be avoided, the accelerated functions can be called
directly by members of the class, inheriting much of the speed
advantages for the compiled code.\footnote{Note that if the speed
  advantage of the individual function comes from vectorization versus
  compilation, the most gain may actually be achieved by simply
  copy-and-pasting the function contents into the class method. See
  Sheppard (2014), Chapter 23, for an excellent overview of Python
  performance and code optimization.} Finally, the HACK library written
as a functional library versus a class-based system allows easy
translation of the HACK library into additional languages if desired.
Julia is a promising target for such an effort; see Sargent and
Stachurski (2015) and their accompanying website for more information on
Julia.

\subsection{Documentation}\label{documentation}

Good documentation is the key to communication between two programmers,
whether between two distinct individuals in the cross section or with
oneself over time. In Python, as in many scripting languages, strings
written on the first line after a function declaration are automatically
employed as system documentation. Two popular style guide for Python
documentation are found in the
\href{http://google-styleguide.googlecode.com/svn/trunk/pyguide.html}{Google
Style Guide} and the \href{https://www.python.org/dev/peps/}{Python
Enhancement Proposals} (PEP) system:
\href{https://www.python.org/dev/peps/pep-0008/}{PEP 8} and
\href{https://www.python.org/dev/peps/pep-0257/}{PEP 257}. HACK
currently uses a slight variation on the PEP 257 style guide. We
illustrate this with a trivial example of a Python documentation string
for a CRRA utility function. The documentation is the section of code
enclosed by the triple quotes:\footnote{Triple quotes in Python set off
  a multi-line string.}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def} \NormalTok{utility(c, gamma):}
    \CommentTok{"""}
\CommentTok{    Return constant relative risk aversion (CRRA) utility of consumption "c"}
\CommentTok{    given risk aversion parameter "gamma."}

\CommentTok{    Parameters}
\CommentTok{    ----------}
\CommentTok{    c: float}
\CommentTok{        Consumption value.}
\CommentTok{    gamma: float}
\CommentTok{        Risk aversion, gamma != 1.}

\CommentTok{    Returns}
\CommentTok{    -------}
\CommentTok{    u: float}
\CommentTok{        Utility.}

\CommentTok{    Notes}
\CommentTok{    -----}
\CommentTok{    gamma cannot equal 1. This constitutes natural log utility; np.log}
\CommentTok{    should be used instead.}
\CommentTok{    """}
    \KeywordTok{return}\NormalTok{( c**(}\FloatTok{1.0} \NormalTok{- gamma) / (}\FloatTok{1.0} \NormalTok{- gamma) )}
\end{Highlighting}
\end{Shaded}

This documentation is now employed by the language in the formal
language help system. If we query the language help files for this
function we will get these results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{>>> utility?}
\NormalTok{Type:        function}
\NormalTok{String form: <function utility at }\BaseNTok{0x7f96b473e6e0}\NormalTok{>}
\NormalTok{File:        ~/workspace/HACKUtilities/<ipython-}\DataTypeTok{input}\DecValTok{-20}\NormalTok{-2bbd1323015e>}
\NormalTok{Definition:  utility(c, gamma)}
\NormalTok{Docstring:}
\NormalTok{Return constant relative risk aversion (CRRA) utility of consumption }\StringTok{"c"}
\NormalTok{given risk aversion parameter }\StringTok{"gamma."}

\NormalTok{Parameters}
\NormalTok{----------}
\NormalTok{c: }\DataTypeTok{float}
    \NormalTok{Consumption value.}
\NormalTok{gamma: }\DataTypeTok{float}
    \NormalTok{Risk aversion, gamma != }\DecValTok{1}\NormalTok{.}

\NormalTok{Returns}
\NormalTok{-------}
\NormalTok{u: }\DataTypeTok{float}
    \NormalTok{Utility.}

\NormalTok{Notes}
\NormalTok{-----}
\NormalTok{gamma cannot equal }\DecValTok{1}\NormalTok{. This constitutes natural log utility; np.log}
\NormalTok{should be used instead.}
\end{Highlighting}
\end{Shaded}

In addition to traditional code documentation, notebook-style interfaces
similar to those in Mathematica and Maple have been developed for Python
and a number of other languages. Programmers, including scientific
programmers, have begun using these notebooks to directly communicate
the ideas behind executable code to one another via html output from
these notebooks, displayed in webpages. The HACK project uses
\href{https://jupyter.org/}{Jupyter}, a language-agnostic, browser-based
notebook system spun off of the the IPython project. An example from the
Jupyter homepage can be seen in Figure \ref{fig:jupyter_example_1}.

\begin{figure}[htbp]
\centering
\includegraphics{jupyter_screenshot.png}
\caption{Jupyter Browser-Based Notebook\label{fig:jupyter_example_1}}
\end{figure}

These notebooks can embed TeX-style mathematics typesetting alongside
text and code for highly expressing scientific programming vignettes.

\subsection{Unit Testing}\label{unit-testing}

Many programs are composed of a number of small functions which
accomplish specific tasks. Testing at the individual function level is
key to ensuring that the overall program executes correctly. This is all
the more important for scientific computing, where a mistake deep in the
code (eg. with a numerical approximation function) may be extremely
difficult to track down. Unit testing is the formal practice whereby
each individual function is directly bundled with a set of tests. Each
test tests a specific input and output pair, examining both ``success''
and ``failure'' states. For example, a log utility function should
return a specific known value for a particular risk aversion and
consumption value, and should fail with a particular error if it is
presented a negative consumption value. Unit tests serve multiple
purposes in code: they cover a wide range of ``reasonable and
representative'' input and output values, and also act to conceptually
illustrate when a bit of code is very complex. If it is very difficult
to test a ``small unit'' of code, that code \emph{may} be best
decomposed into smaller and more specific-purpose functions.\footnote{Note
  that there is a tradeoff between performance and decomposition of code
  into smaller and smaller units. This is discussed in Sheppard
  (2014)\href{REF}{}.}

In scientific programming, this can serve an additional purpose: peer
review of code. Uncovering bugs in code, even one's own code, can be
notoriously difficult. This is many times more true when one is
examining the code written by another. Thus scientific peer review of
code is nearly prohibitively costly, and very difficult to undertaken in
a structured fashion. Unit testing can ease the burden of scientific
code review in at least two ways. First, it can aid documentation in
immediately outlining simple examples of code execution. Second, it can
outline the pitfalls and testing procedures a reviewer may want to
undertake to ensure that the code is correct. Instead of starting with a
``blank page,'' a reviewer can take the unit tests written by the
original author, run them, and then (assuming they all pass), examine
the tests to see if any particular cases appear to be excluded. If so,
the reviewer can use the unit tests as a template to quickly write
another test case and run that as well. This can greatly accelerate both
the verification of work done, as well as new testing of the code, all
in a well-established and minimally costly framework.

In Python there are two built-in ways to write tests for a function:
internally to the documentation, in a
``\href{https://docs.python.org/2/library/doctest.html}{doctest},'' and
externally in a more formal unit testing framework,
``\href{https://docs.python.org/2/library/unittest.html}{unittest}.''
Using the utility function defined earlier above, we add a doctest to
the end of the function documentation (removing earlier documentation
for brevity). The tests are denoted by the triple right-caret under the
heading denoted ``Tests.'' The appropriate output of the test is denoted
in the line directly below the caretted line, and we will use the
doctest library to run these tests. First the code definition:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def} \NormalTok{utility(c, gamma):}
    \CommentTok{"""}
\CommentTok{    Return CRRA utility of consumption "c" given risk aversion parameter "gamma."}

\CommentTok{    ...(excluded for brevity)...}

\CommentTok{    Tests}
\CommentTok{    -----}
\CommentTok{    Test a value which should pass:}
\CommentTok{    >>> utility(1.0, 2.0)}
\CommentTok{    -1.0}

\CommentTok{    Test a value which should fail:}
\CommentTok{    >>> utility(1.0, 1.0)}
\CommentTok{    Traceback (most recent call last):}
\CommentTok{        ...}
\CommentTok{    ZeroDivisionError: float division by zero}
\CommentTok{    """}
    \KeywordTok{return}\NormalTok{( c**(}\FloatTok{1.0} \NormalTok{- gamma) / (}\FloatTok{1.0} \NormalTok{- gamma) )}
\end{Highlighting}
\end{Shaded}

We save this code in a file called ``utility.py.'' The code file now
constitutes a Python
\href{https://docs.python.org/2/tutorial/modules.html}{module}, and we
use the doctest library to execute the tests embedded in the doctring.
We execute the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{>>> }\CharTok{import} \NormalTok{utility      }\CommentTok{# Import the new utility module}
\NormalTok{>>> }\CharTok{import} \NormalTok{doctest      }\CommentTok{# Import built-in doctest module}
\NormalTok{>>>}
\NormalTok{>>> doctest.testmod(utility, verbose=}\OtherTok{True}\NormalTok{)  }\CommentTok{# Execute doctest on utility}
\NormalTok{Trying:}
    \NormalTok{utility(}\FloatTok{1.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{)}
\NormalTok{Expecting:}
    \NormalTok{-}\FloatTok{1.0}
\NormalTok{ok}
\NormalTok{Trying:}
    \NormalTok{utility(}\FloatTok{1.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{)}
\NormalTok{Expecting:}
    \NormalTok{Traceback (most recent call last):}
        \NormalTok{...}
    \OtherTok{ZeroDivisionError}\NormalTok{: }\DataTypeTok{float} \NormalTok{division by zero}
\NormalTok{ok}
\DecValTok{1} \NormalTok{items had no tests:}
    \NormalTok{utility}
\DecValTok{1} \NormalTok{items passed }\DataTypeTok{all} \NormalTok{tests:}
   \DecValTok{2} \NormalTok{tests in utility.utility}
\DecValTok{2} \NormalTok{tests in }\DecValTok{2} \NormalTok{items.}
\DecValTok{2} \NormalTok{passed and }\DecValTok{0} \NormalTok{failed.}
\NormalTok{Test passed.}
\end{Highlighting}
\end{Shaded}

We see that the two tests passed. A contributor or a reviewer can
quickly run these tests on new code, and quickly add new tests if
needed. More complicated tests can be executed with the unittest
framework, which is not discussed here in depth.

\subsection{Language-agnostic, Human-Readable Data
Serialization}\label{language-agnostic-human-readable-data-serialization}

Consider the following scenario: a researcher wants to replicate a
computational model. After endless work and testing, the results between
two codebases simply cannot be reconciled. Many hours are spent hunting
for bugs until it is finally discovered that the problem is not in the
code, but rather in a small mistake transcribing parameter settings. For
some (most?) this can be an all-too-familiar experience.

One way to avoid this is using the exact same parameter settings file
for all possible code-bases. One language-independent file is used to
store all parameters and calibration settings for a model.\footnote{Not
  including the data for fitting the model -- this may easily be very
  large and is stored separately.} A replication of a particular model
can use that single parameter file to confidently reproduce results
across implementations. The parameter file should be easily readable by
a human, as this is one more place mistakes may occur and the easier to
double-check, the better. Calibration of course may require many
different types of data objects contained together in a single setting
-- floating point numbers, strings, booleans, even vectors or arrays of
values. Flat-file data formats such as CSV are not flexible enough to
handle all these types well. Fortunately, modern software developers
have already addressed this problem with a number of options. The HACK
project uses \href{http://json.org/}{JSON} (JavaScript Object Notation),
a data structure somewhat analogous to simplified XML. The contents of a
small JSON file may look like the following. Note the ability to include
vectors, strings, and boolean values in the same file:\footnote{The
  values used in the examples in this paper are illustrative and not
  used for a particular estimation exercise, unless otherwise noted.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\{}
\DataTypeTok{"rho"}\NormalTok{:   }\FloatTok{3.0}\NormalTok{,}
\DataTypeTok{"beta"}\NormalTok{:  }\FloatTok{0.99}\NormalTok{,}
\DataTypeTok{"R"}\NormalTok{:     }\FloatTok{1.03}\NormalTok{,}
\DataTypeTok{"liquidity_constraint"}\NormalTok{: }\DecValTok{true}\NormalTok{,}
\DataTypeTok{"interpolation_type"}\NormalTok{: }\StringTok{"linear"}\NormalTok{,}
\DataTypeTok{"psi_sigma"}\NormalTok{: [}\FloatTok{0.001}\NormalTok{, }\FloatTok{0.001}\NormalTok{, }\FloatTok{0.001}\NormalTok{,}
    \FloatTok{0.28}\NormalTok{, }\FloatTok{0.27}\NormalTok{, }\FloatTok{0.27}\NormalTok{, }\FloatTok{0.26}\NormalTok{, }\FloatTok{0.26}\NormalTok{,}
    \FloatTok{0.25}\NormalTok{, }\FloatTok{0.24}\NormalTok{, }\FloatTok{0.23}\NormalTok{, }\FloatTok{0.22}\NormalTok{, }\FloatTok{0.21}\NormalTok{],}
\DataTypeTok{"Gamma"}\NormalTok{:  [ }\FloatTok{1.0}\NormalTok{,   }\FloatTok{1.0}\NormalTok{,   }\FloatTok{0.7}\NormalTok{,}
\FloatTok{1.01}\NormalTok{,  }\FloatTok{1.01}\NormalTok{,  }\FloatTok{1.01}\NormalTok{,  }\FloatTok{1.01}\NormalTok{,  }\FloatTok{1.01}\NormalTok{,}
\FloatTok{1.025}\NormalTok{, }\FloatTok{1.025}\NormalTok{, }\FloatTok{1.025}\NormalTok{, }\FloatTok{1.025}\NormalTok{, }\FloatTok{1.025}\NormalTok{],}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The HACK project uses a single JSON file to store parameters and
calibration values which can be used across multiple implementations of
a model, even in multiple languages. For example, the same JSON file can
be read by both a MATLAB and Python implementation of the same
model.\footnote{A file which reads in the parameters and sets up the
  environment will of course be required for each language, and the
  researcher must be careful to treat parameters equivalently in this
  setup step.}

\subsection{Application Programming Interface
(API)}\label{application-programming-interface-api}

When contributing a module or a function to a larger code library, a
programmer needs to know how this function or module fits into the
overall framework of the codebase. A strict specification of variable
inputs and outputs for a function communicates this information. This is
the so-called Application Programming Interface. Any large computational
project with multiple developers can benefit from such an interface.
Some languages such as Java cleave to the API approach at a deep level,
in the language definition itself, while others have a looser approach
to API definition. In Python an API can be formally defined in a number
of ways, or not at all. The HACK project forms a very simple API for the
codebase, organized around the modules required to run a
partial-equilibrium or general-equilibrium estimation.

Another use of APIs is to define an interface between any language and
particular datasets. This second use of APIs, the database use, is just
as important as its usage in organizing code. Given the vast differences
in different microeconomic database structures, this is very difficult
utility to create for broad use. Organizations such as the
\href{http://openeconomics.net/}{Open Economics Working Group} may
provide a unified approach for economic data.

\subsection{Version Control}\label{version-control}

An essential tool in distributed software development is a system which
can automatically archive versions of code, as well as allow the merging
of changes to a document by two different programmers. Such a system is
known as a version control system. The HACK project uses the Git version
control system, and uses the popular online repository service Github to
archive its codebase. Chacon and Straub (2014) is an excellent overview
of version control in general and Git and Github in particular. Github
allows code to be posted to a single online repository which tracks
previous versions. A repository is copied to the computer of each
contributor, and the central code source on Github may be kept private,
accessible only to select users, or made widely available to the general
public. Github provides a number of services on top of the pure
repository service, including a simple wiki space, a space for a static
website, and simple one-off repositories called ``Gists,'' which allow
the quick public or private posting of a variety of content, including
Jupyter notebooks.\footnote{See this Github blogpost,
  ``\href{https://github.com/blog/1995-github-jupyter-notebooks-3}{GitHub
  + Jupyter Notebooks = \textless{}3}'', which explicitly outlines the
  use of Github for sharing notebooks.}

\subsection{Bringing It Together: Reproducible
Research}\label{bringing-it-together-reproducible-research}

Many of the tools above are used to create research which can be
immediately reproduced, even entirely in a web browser. This
\href{https://github.com/ipython/ipython/wiki/A-gallery-of-interesting-IPython-Notebooks}{gallery
of interesting IPython Notebooks} outlines a number of research projects
which combine code, discussion, data visualization, and descriptive
mathematics to make science as transparent and reproducible as possible.
For example Ram and Hadany (2015) reproduce a section of their work in
an IPython notebook, which can be found
\href{http://nbviewer.ipython.org/url/www.sciencedirect.com/science/MiamiMultiMediaURL/1-s2.0-S0040580914000811/1-s2.0-S0040580914000811-mmc1.txt/272364/FULL/S0040580914000811/471cf02085a52c248dc76ae65ad4409d/mmc1.txt}{here},
and excerpt of which can be seen in Figures \ref{fig:Ram_Hadany_1},
\ref{fig:Ram_Hadany_2}, \ref{fig:Ram_Hadany_3}.

\begin{figure}[htbp]
\centering
\includegraphics{jupyter_screenshot_reproducible_research_1.png}
\caption{Ram and Hadany (2015) Notebook Excerpt
1\label{fig:Ram_Hadany_1}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics{Screenshot-image1.png}
\caption{Ram and Hadany (2015) Notebook Excerpt
2\label{fig:Ram_Hadany_2}}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics{Screenshot-image2.png}
\caption{Ram and Hadany (2015) Notebook Excerpt
3\label{fig:Ram_Hadany_3}}
\end{figure}

Many additional examples of reproducible research are available in the
gallery noted above.

\section{Methodological Framework}\label{methodological-framework}

\href{of\%20the\%20HACK\%20Framework}{}

The foundational agent for the HACK toolkit is the microeconomic
rational consumer. The agent's problem is stated as a dynamic stochastic
optimization problem \href{with\%20a\%20unique\%20solution\%20and}{}
which is solved via dynamic programming. Given the solution method and
appropriate data, the model is then estimated via Simulated Method of
Moments (SMM)\href{INSERT\%20REFS\%20HERE!}{}, with standard errors
obtained via the
bootstrap\href{probably\%20need\%20a\%20further\%20reference\%20here,\%20and\%20a\%20subsection\%20or\%20footnote\%20on\%20robustness\%20of\%20the\%20bootstrap}{}.
The key is to write the code such that there is a logical division
between elements of the solution \href{and\%20estimation}{} method. The
ideal solution method decomposition should allow the various modules of
the code to be agnostic to one another -- if one module is replaced by a
different module, which simply takes the same appropriate inputs and
outputs, the solution works as before. To use a software term, the HACK
project defines an API (Application Programming Interface) which
instructs the user in how different solution modules communicate with
one another, regardless of what they do internally. We call an element
of the solution method ``modular'' is this is particularly easy to do.
For example, as described further below, the Simulated Method of Moments
estimation procedure can be robust in this way -- under broad
conditions, it can be applied to a very wide range of dynamic decision
models\footnote{The choices represented in HACK are not the only modular
  solution methods which may be used, but rather a baseline. If you have
  a favorite solution method which you believe is robust and modular as
  described here, you are encouraged to contribute!}.

One final note before proceeding\href{continuing}{}. This modular
approach aligns well with the authors' strategy of implementing
non-optimal behavior as a departure from an already-established
optimizing framework. If agents are to learn, the first straightforward
extension is to learn in about some element of the solution method --
eg. learn expectations or learn the optimal policy. If an agent makes
mistakes, there are clear places to implement these mistakes in the
optimizing framework: in the expectation function, in the law of motion,
in following the optimal policy. In the codebase of HACK, the optimal
problem is developed first as the framework upon which further
extensions are hung. Implementing a non-optimizing solution thus
involves augmenting or extending a bit of the baseline code.
\href{replaces\%20depart\%20and\%20then\%20portions\%20of\%20the\%20solution\%20are}{}
\href{belief\%20that\%20the\%20most\%20straightforward\%20way\%20to\%20implement\%20non-optimizing\%20behavior\%20in\%20an\%20economic\%20model\%20is\%20through\%20specific\%20departures\%20from\%20an\%20already-established\%20optimizing\%20framework.}{}

This rest of this section outlines the basic optimization problem and
solution method which forms the foundation of the HACK framework. The
solution method is decomposed into a few major conceptual parts, which
are implemented as modular libraries in HACK. Additional unimplemented
solution methods are discussed. At each stage, the modular nature of the
methods are noted. \href{CONFIRM}{}

\subsection{A Basic Partial-Equilibrium
Example}\label{a-basic-partial-equilibrium-example}

Consider the following finite-horizon consumption-under-uncertainty
problem.\footnote{See Carroll (2014) for much more background and detail
  on this style of problem.} At time $T+1$, the consumer dies with
certainty. The problem is to allocate consumption appropriately from
$t=0$ to $t=T$. The full problem from Carroll (2012a) is: \href{REF}{}

\[
\begin{aligned}
\underset{\{\mathbf{c}_{t+j} \} _{j=0} ^{\infty} }{max} \; & \mathbb{E}_{t} \left[ \sum _{j=0} ^{T-t} \beta^{j} u(\mathbf{c}_{t+j}) \right] \\
& s.t. \\
\mathbf{a}_{t} & = \mathbf{m_t} - \mathbf{c_t} \\
\mathbf{b}_{t+1} & = \mathbf{a_t}\mathsf{R} \\
\mathbf{p}_{t+1} & = \mathbf{p_
t} \Gamma \psi_{t+1} = \mathbf{p_t} \Gamma_{t+1} \\
\mathbf{m}_{t+1} & = \mathbf{b}_{t+1} + \mathbf{p}_{t+1} \xi_{t+1} \\
& \mathbf{m}_0\;given
\end{aligned}
\]

where

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  $\mathbf{a}_t$ is end-of-period assets,
\item
  $\mathbf{m}_t$ is beginning-of-period total market resources (``cash
  on hand''),
\item
  $\mathbf{c}_t$ is consumption in period $t$,
\item
  $\mathsf{R}$ is a constant return factor on assets,
  $\mathsf{R}=(1+r)$,
\item
  $\mathbf{p}_{t}$ is permanent non-asset income,
\item
  $\Gamma$ is a constant permanent income growth factor,
\item
  $\psi_{t}$ is a mean-1 iid permanent shock to income, and
\item
  $\xi_{t}$ is a mean-1 iid transitory shock income, composed as
\end{itemize}

\[\xi_{t}=\begin{cases}
0 & with\; prob\;\wp_t>0\\
\tfrac{\theta_{t}}{\not\wp_t} & with\; prob\;\not{\wp_t} \equiv(1-\wp_t)
\end{cases}, \textrm{where}\]

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  $\wp_t$ is a small probability that income will be zero
\item
  $\theta_t$ is a mean-1 iid shock transitory to income
\end{itemize}

This setup can describe a wide range of consumer circumstances,
including retirement and fixed income over final years of life.

The utility function $u(.)$ is of the Constant Relative Risk Aversion
(CRRA) form with risk-aversion parameter $rho$:

\[u(c) = \frac{c^(1-\rho)}{1-\rho}.\]

As in Carroll (2012a), this problem can be normalized by permanent
income $\mathbf{p}_{t}$ to produce a simplified version of the full
problem, with a reduced number of state variable. The bold symbols used
above indicate non-normalized variables, while the regular non-bold
symbols used below indicate variables normalized by permanent
income.\footnote{See Carroll (2012a) for the details of this process.}
The normalized problem can be written in Bellman form:

\[
\begin{aligned}
v_t(m_t) & = \underset{c_{t}}{max} \; u(c_t) + \beta \mathbb{E}_{t} \left[ \Gamma_{t+1}^{1-\rho} v_{t+1}(m_{t+1}) \right] \\
& s.t. \\
a_{t} & = m_t - c_t \\
b_{t+1} & = \left( \frac{\mathsf{R}}{\Gamma_{t+1}} \right) a_t = \mathcal{R}_{t+1} a_t \\
m_{t+1} & = b_{t+1} + \xi_{t+1} \\
& m_0\;given
\end{aligned}
\]

or simplified further:

\[
\begin{aligned}
v_t(m_t) & = \underset{c_{t}}{max} \; u(c_t) + \beta \mathbb{E}_{t} \left[ \Gamma_{t+1}^{1-\rho} v_{t+1}(m_{t+1}) \right] \\
& s.t. \\
m_{t+1} & = \mathcal{R}_{t+1}(m_t - c_t) + \xi_{t+1} \\
& m_0\;given
\end{aligned}
\]

\subsection{The Solution Method}\label{the-solution-method}

The general solution method is as follows: in the final period $T$, the
value function in the following period is $v_{T+1}(m) = 0 \; \forall_m,$
and the value function in period $T$ is simply $v_{T}(m) = u(m).$ This
makes the problem in period $T-1$ straightforward to solve numerically
for both the consumption function and value functions $c_{T-1}^{*}(m)$
and $v_{T-1}^{*}(m)$:

\[
\begin{aligned}
c_{T-1}^{*}(m) & = \underset{c \in [0, \bar{m}]}{argmax} \; u(c) + \beta \mathbb{E}_{T-1} \left[ \Gamma_{T}^{1-\rho} u(\mathcal{R}_{T}(m - c) + \xi_{t+1}) \right] \\
& \mathrm{and} \\
v_{T-1}^{*}(m) & = u(c_{T-1}^{*}(m)) + \beta \mathbb{E}_{T-1} \left[ \Gamma_{T}^{1-\rho} u(\mathcal{R}u(c_{T-1}^{*}(m)) \right]
\end{aligned}
\]

where $\bar{m}$ is a self-imposed liquidity constraint.\footnote{Carroll
  (2012a) demonstrates the reasoning behind this derivation. In a model
  with positive probability of a zero-income event, $\bar{m}=m$.}

With these numerical solutions in hand, the solution method is now
simply recursive: step back one more period to $T-2$ and solve for
optimal consumption and value functions using $c_{T-1}^{*}(m)$ and
$v_{T-1}^{*}(m)$. This process can be continued back until the first
period $t=0$. This solution process is outlined in greater detail in
Carroll (2012a).

\subsection{The Estimation Method}\label{the-estimation-method}

Denote the behavioral parameters $\beta, \rho$, (discounting and risk
aversion, respectively) as

\[\phi = \{\beta, \rho\}\]

and denote the structural problem parameters as

\[
\begin{aligned}
\varrho & = \{ \varrho_{t} \}_{t=0} ^{T}, \mathrm{ where } \\
\varrho_{t} & = \{ \Gamma, \psi_{t}, \xi_{t}, \wp_t, \theta_t \}, \forall_t .
\end{aligned}
\]

Given an arbitrary behavioral parameter set $\phi = \{\beta, \rho\}$,
and choosing the values and data-generating processes for the structural
problem parameters $\varrho$ to match consumer experiences in the PSID,
we can solve for the set of consumption functions which are optimal
under these conditions, $\{ c_{t}^{*}(m) \}_{t=0}^{T}.$

With these consumption functions now in hand, we can use the calibrated
parameters $\varrho$ to generate $N$ different simulated consumer
experiences (vectors of income shocks) from $t=0,1,...,T$. Applying the
consumption functions $\{ c_{t}^{*}(m)\}_{t=0}^{T}$ to this set of
simulated experiences generates a $N$-sized distribution of simulated
wealth holdings for all $t$. The moments of these cross-sectional
distributions of wealth can then be compared to the equivalent moments
in appropriately constructed empirical data from the Survey of Consumer
Finance (SCF). We form the following objective function, which compares
population median between empirical wealth-to-income ratio from the SCF
and its simulated equivalent:

\[\varpi_{\varrho}(\phi) \equiv \sum_{i=1}^{N} \omega_{i}|\zeta_{i}^{\tau}-\mathbf{s}^{\tau}_{\varrho}(\phi)|.\]

Here $\varpi_{\varrho}(\phi)$ represents the objective value for the
distance between medians of the two populations, the synthetic
population variables represented by $\mathbf({s})$ and the empirical
population variables represented by $\zeta$ (see Carroll 2012a for more
discussion of the form of this objective function for population
moments). The index $i$ indicates individual observations in the
empirical data, each of which has a population weight $\omega_i$
(required in the SCF due to oversampling of particular sub-populations).
Each individual $i$ in the empirical data has observations at the
age-group frequency, $\tau$. The variable
$\mathbf{s}_{\varrho}^\tau(\phi)$ is the median of the simulated data
for age group $\tau$, under calibration $\varrho$, using the parameters
$\phi = \{\beta, \rho\}$. Once this value has been constructed as a
function of $\phi$, the estimation occurs by simply finding the minimal
$\phi$ value numerically:

\[\underset{\phi}{min} \; \varpi_{\varrho}(\phi).\]

This is accomplished in code by simply handing the expression
$\varpi_{\varrho}(\phi)$ to a numerical minimization process. The
standard error on the resulting estimation of $\{\beta^{*}, \rho^{*}\}$
is found by bootstrapping the empirical data and repeating the above
estimation process a number of times, $N_{bootstrap}$.

\subsection{Modular Solution and Estimation in
HACK}\label{modular-solution-and-estimation-in-hack}

The solution and estimation method described for the basic problem above
can be decomposed into the following steps, each of which is written as
a module in Python. Each module is documented, tested, and brought
together in IPython notebook ``vignettes'' to demonstrate their use, and
finally brought together in a simple interface to effect model solution
and estimation. Extending the partial-equilibrium toolkit corresponds to
writing a new version of a specific set of functions in each of the
basic modules, using the exiting code and vignettes as examples and
guides.

The major conceptual solution and estimation components for the
partial-equilibrium problem are:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  parameter definition
\item
  expectations formation and calculation
\item
  value and policy formation
\item
  simulation of population experience under a particular policy
\item
  estimation of parameters using SMM and bootstrapping
\end{itemize}

These parts together form the basis of the partial-equilibrium portion
of HACK. Code is divided into the following primary modules,
corresponding to the solution method breakdown noted above:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  SetupParameters.py
\item
  HACKUtilities.py
\item
  SolutionLibrary.py
\item
  SimulationLibrary.py
\item
  Estimation.py
\end{itemize}

This section examines these basic modules and outlines the process by
which the library can be expanded to include additional models. Work is
underway to build out the general-equilibrium portion using the approach
implemented in Carroll et al. (2015). Namely, the following additions
will be made:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  price-finding via market clearing
\item
  rational expectations via the Krusell-Smith (1998) algorithm.
\end{itemize}

The rest of this section outlines the main contents of the five modules
noted above. Each is illustrated with pseudo-code headers and content as
needed. For each module, the primary functions which need to be modified
to extend the baseline model are identified and discussed.

\subsubsection{SetupParameters.py}\label{setupparameters.py}

This module is coupled with an input JSON file, which specifies the full
set of calibrated values required to solve the model. The JSON file, as
noted previously, is language-independent, and can be read and used by
nearly any modern programming language. The HACK project has used this
in particular to validate multiple-language versions of the same model,
eg. between MATLAB and Python. This setup allows calibration parameters
to be specified once, in a separate, easily human-readable file.
Importantly, the SetupParameters also executes a key function for the
estimation step: it brings in and organizes the empirical data to be
used in the estimation process,\footnote{This data is usually stored in
  a separate format than the parameters in the JSON file.} and it
defines a function \emph{find\_simulated\_medians} which will take in
simulated wealth data and organizes it to be comparable to the empirical
data as stated in the SMM objective expression,

\[ \sum_{i=1}^{N} \omega_{i}|\zeta_{i}^{\tau}-\mathbf{s}^{\tau}_{\varrho}(\phi)|.\]

When the user desires to change the estimation procedure (eg. change the
empirical data or moments compared), the SetupParamters file must be
changed appropriately.

This SetupParameters file is imported into any subsequent module which
needs to access the parameters using the following line of code. Note
that particular parameters are immediately accessible:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{>>> }\CharTok{import} \NormalTok{SetupParameters }\CharTok{as} \NormalTok{param}
\NormalTok{>>>}
\NormalTok{>>> }\DataTypeTok{print} \StringTok{"R ="}\NormalTok{, param.R}
\NormalTok{R = }\FloatTok{1.03}
\NormalTok{>>> }\DataTypeTok{print} \StringTok{"Gamma ="}\NormalTok{, param.Gamma}
\NormalTok{array([ }\DecValTok{1}\NormalTok{.   ,  }\DecValTok{1}\NormalTok{.   ,  }\DecValTok{1}\NormalTok{.   ,  }\DecValTok{1}\NormalTok{.   ,  }\DecValTok{1}\NormalTok{.   ,  }\DecValTok{1}\NormalTok{.   ,  }\DecValTok{1}\NormalTok{.   ,  }\DecValTok{1}\NormalTok{.   ,}
        \FloatTok{0.7}  \NormalTok{,  }\FloatTok{1.01} \NormalTok{,  }\FloatTok{1.01} \NormalTok{,  }\FloatTok{1.01} \NormalTok{,  }\FloatTok{1.01} \NormalTok{,  }\FloatTok{1.01} \NormalTok{,  }\FloatTok{1.01} \NormalTok{,  }\FloatTok{1.01} \NormalTok{,}
        \FloatTok{1.025}\NormalTok{,  }\FloatTok{1.025}\NormalTok{,  }\FloatTok{1.025}\NormalTok{,  }\FloatTok{1.025}\NormalTok{,  }\FloatTok{1.025}\NormalTok{,  }\FloatTok{1.025}\NormalTok{,  }\FloatTok{1.025}\NormalTok{,  }\FloatTok{1.025}\NormalTok{,}
        \FloatTok{1.025}\NormalTok{])}
\NormalTok{>>> }\DataTypeTok{print} \StringTok{"Initial beta guess for SMM optimization:"}\NormalTok{, param.beta_start}
\NormalTok{Initial beta guess }\KeywordTok{for} \NormalTok{SMM optimization: }\FloatTok{0.99}
\end{Highlighting}
\end{Shaded}

Note that the JSON file includes initial guesses for the parameters to
be estimated by the Simulated Method of Moments routine. To extend the
baseline HACK model, new calibrated parameter values will need to be
added to this file.

\subsubsection{HACKUtilities.py}\label{hackutilities.py}

This module contains a number of utilities used by the HACK framework,
including the code to implement agent expectations. Agent expectations
here are implemented as a discretization of the shock-space, achieved by
choosing the size of discrete points to represent the distribution,
$N_{discrete}$, creating an equiprobably-spaced partition over the
support, and selecting the representative point in each partition as the
conditional mean of values in the partition. Each resultant point is
then assigned the probability $\frac{1}{N_{discrete}}$. See Carroll
(2012a) for a detailed discussion of this approach.

An example of the code which implements a mean-1 lognormal shock space
is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def} \NormalTok{calculate_mean_one_lognormal_discrete_approx(N, sigma):}
    \CommentTok{'''}
\CommentTok{    Calculate a discrete approximation to a mean-1 lognormal distribution.}

\CommentTok{    Parameters}
\CommentTok{    ----------}
\CommentTok{    N: float}
\CommentTok{        Size of discrete space vector to be returned.}
\CommentTok{    sigma: float}
\CommentTok{        standard deviation associated with underlying normal probability distribution.}

\CommentTok{    Returns}
\CommentTok{    -------}
\CommentTok{    X: np.ndarray}
\CommentTok{        Discrete points for discrete probability mass function.}
\CommentTok{    pmf: np.ndarray}
\CommentTok{        Probability associated with each point in X.}

\CommentTok{    Test}
\CommentTok{    ----}
\CommentTok{    Confirm that returns discrete mean of 1}
\CommentTok{    >>> import numpy as np}
\CommentTok{    >>> x, pmf = calculate_mean_one_lognormal_discrete_approx(N=5, sigma=0.2)}
\CommentTok{    >>> np.dot(x,pmf)}
\CommentTok{    1.0}
\CommentTok{    '''}
    \NormalTok{mu = -}\FloatTok{0.5}\NormalTok{*(sigma**}\DecValTok{2}\NormalTok{)}
    \NormalTok{distrib = stats.lognorm(sigma, }\DecValTok{0}\NormalTok{, np.exp(mu))}

    \CommentTok{# ------ Set up discrete approx ------}
    \NormalTok{pdf = distrib.pdf}
    \NormalTok{invcdf = distrib.ppf}
    \NormalTok{probs_cutoffs = np.arange(N}\FloatTok{+1.0}\NormalTok{)/N      }\CommentTok{# Includes 0 and 1}
    \NormalTok{state_cutoffs = invcdf(probs_cutoffs)   }\CommentTok{# State cutoff values, each bin}

    \CommentTok{# Set pmf:}
    \NormalTok{pmf = np.repeat(}\FloatTok{1.0}\NormalTok{/N, N)}

    \CommentTok{# Find the E[X|bin] values:}
    \NormalTok{F = }\KeywordTok{lambda} \NormalTok{x: x*pdf(x)}
    \NormalTok{Ebins = []}

    \KeywordTok{for} \NormalTok{i, (x0, x1) in }\DataTypeTok{enumerate}\NormalTok{(}\DataTypeTok{zip}\NormalTok{(state_cutoffs[:-}\DecValTok{1}\NormalTok{], state_cutoffs[}\DecValTok{1}\NormalTok{:])):}
        \NormalTok{cond_mean1, err1 = quad(F, x0, x1, epsabs=}\FloatTok{1e-10}\NormalTok{, epsrel=}\FloatTok{1e-10}\NormalTok{, limit=}\DecValTok{200}\NormalTok{)}
        \CommentTok{# Note that the *first* to be fulfilled of epsabs and epsrel stops the}
        \CommentTok{# integration - be aware of this when the answer is close to zero.}
        \CommentTok{# Also, if you never care about one binding (eg if one would like to}
        \CommentTok{# force scipy to use the other condition to integrate), set that = 0.}
        \NormalTok{Ebins.append(cond_mean1/pmf[i])}

    \NormalTok{X = np.array(Ebins)}

    \KeywordTok{return}\NormalTok{( [X, pmf] )}
\end{Highlighting}
\end{Shaded}

Note the embedded doctest, which runs ``sanity checks'' on the
discretization process. Tests such as these identified initial numerical
errors in the discretization process due to loose default integration
tolerances -- a key contribution of unit testing which can help avoid
many hours of bug hunting in incorrect portions of the codebase.
\href{In\%20addition\%20to\%20expectation-formation,\%20this\%20module\%20implements\%20functions\%20to\%20estimate\%20the\%20welfare\%20loss\%20due\%20to\%20function\%20approximation,\%20as\%20employed\%20in\%20Allen\%20and\%20Carroll\%20(2001),\%20and\%20functions\%20to\%20implement\%20variance-reduction\%20methods\%20for\%20the\%20Monte\%20Carlo\%20simulation\%20portion\%20of\%20the\%20SMM\%20routine.}{}

The key modular feature of the HACKUtilities library is that it
produces, finally, a single discrete representation of the probability
space faced by the consumer. As long as shocks are iid, the number of
dimensions of shocks does not matter -- each distribution is discretized
and the joint distribution is create combinatorically from the
individual discrete marginal distributions. To create expectations, the
HACKUtilities library finally produces a set of combinations of all
discrete points as the support, and the corresponding combination of all
discrete probabilities as the distribution. Expectations of a function
$f$ are then formed simply by the dot product of $f$ applied to each
point with the probabilities associated with all points.

If additional methods of expectations formation are desired, this is the
correct module in which to develop them.
\hyperref[ux5cux23ux5cux2520Modelux5cux2520Extensionux5cux2520-ux7bux7d-ux5cux2520SECTIONux5cux2520TOux5cux2520ADD.ux5cux2520PERHAPSux5cux2520AFTERux5cux2520WRITINGux5cux2520THISux5cux2520WEEKux5cux2520FORux5cux2520RXux3f]{}

\subsubsection{SolutionLibrary.py}\label{solutionlibrary.py}

The solution library contains the main definitions used in the solution
method. The HACK project uses dynamic programming to determine the
solution to the consumer problem, and in particular uses the endogenous
gridpoints method to greatly accelerate solving for the policy
function.\footnote{The endogenous gridpoints method is discussed in
  extensive detail in Carroll (2006, 2012a) \href{REF}{}} This solution
method takes advantage of the \emph{end of period} consumption and value
functions,

\[\mathfrak{c}(a_t) \;\; \mathrm{ and } \;\;\mathfrak{v}(a_t) \]

which map end-of-period wealth $a_t$ to consumption and expected value.
The endogenous gridpoints method is not required for the HACK project,
but it greatly accelerates the solution method and is used in the
baseline HACK model. This is one example of including concrete examples
of non-trivial computational ``tricks'' which may greatly improve a
solution method. The endogenous gridpoints method may not be easy to
understand upon first encounter. Thus the HACK toolkit includes it along
with an IPython notebook which quickly illustrates how the process works
-- an interactive and executable summary of Carroll (2012a).
\href{REF}{}

The key methods in the SolutionLibrary module are the following, shown
only with their function headers -- documentation and code details are
excluded for brevity. The main functions are:

\textbf{Utility functions:} The CRRA utility function is defined with
its first and second derivative:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def} \NormalTok{utility(c, gamma):}
    \KeywordTok{pass}

\KeywordTok{def} \NormalTok{utilityP(c, gamma):}
    \KeywordTok{pass}

\KeywordTok{def} \NormalTok{utilityPP(c, gamma):}
    \KeywordTok{pass}
\end{Highlighting}
\end{Shaded}

\textbf{The end-of-period consumption function for period $T-1$ and all
other $t < T-1$} These are the key functions used in the endogenous
gridpoints backwards induction method:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def} \NormalTok{gothicC_Tm1(a, rho, uP, R, beta, Gamma, psi_support, xi_support, pmf):}
    \KeywordTok{pass}

\KeywordTok{def} \NormalTok{gothicC_t(a, c_prime, rho, uP, R, beta, Gamma, psi_support, xi_support, pmf):}
    \KeywordTok{pass}
\end{Highlighting}
\end{Shaded}

\textbf{Initialize the consumer problem:} A consumer's problem must be
set up before it can be solved: the expectations support and probability
mass function must be created for each period:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def} \NormalTok{init_consumer_problem(R, Gamma, constrained,}
                          \NormalTok{psi_sigma, psi_N, xi_sigma, xi_N, ...):}
    \KeywordTok{pass}
\end{Highlighting}
\end{Shaded}

\textbf{Solve the consumer problem:} The recursive solution method is
implemented by two functions: the first solves a single period in the
problem (``one step back''), while the second implements the full
backwards recursion, from period $T-1$ to $0$:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def} \NormalTok{step_back_one_period(rho, beta, R, Gamma, shocks, pmf, a_grid):}
    \KeywordTok{pass}

\KeywordTok{def} \NormalTok{solve_total_consumption_problem(rho, beta, R, Gamma, shocks, pmf, a_grid):}
    \KeywordTok{pass}
\end{Highlighting}
\end{Shaded}

The final function, ``\emph{solve\_total\_consumption\_problem},''
returns a list of consumption function objects, ordered in reverse
chronology (index 0 is the consumption function for period $T$, index 1
is consumption function for $T-1$, etc.). All other functions in the
SolutionLibrary module can be thought of as supporting the final
\emph{solve\_total\_consumption\_problem} function. Thus the baseline
model can be extended by overwriting the
\emph{solve\_total\_consumption\_problem} function, as well as
creating/overwriting whatever additional functions are needed to support
the new implementation. For example, if a model is extended with respect
to solution methods -- for example, if Bayesian learning is added to the
agent solution method -- this module is the correct place to include
that extension.

\subsubsection{SimulationLibrary.py}\label{simulationlibrary.py}

The simulation library implements the simulation step of the estimation
process -- given a reverse-chronology list of consumption functions, the
functions in this library will draw an appropriate panel of shocks of
size $\{T, N_{simulate}\}$, where $T$ is total periods and
$N_{simulate}$ is the total number of agents for which to simulate
experiences (eg. $T=60$ and $N_{simulate}=10,000$).

The key methods in the SimulationLibrary module are the following, again
shown only with function headers for brevity:

\textbf{Main two functions:} The main two functions in the
SimulationLibrary module first create the complete set of shocks
required to simulate agent experiences and, second, apply the
consumption solution from the SolutionLibrary to this set of shocks to
produce the simulated wealth panel. To extend the baseline model, these
are the two major functions to overwrite, as well as the requisite
helper functions. The two major functions appear as follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def} \NormalTok{create_income_shocks_experience(psi_sigma, xi_sigma, Gamma, R, p_unemploy, T, N):}
    \KeywordTok{pass}

\KeywordTok{def} \NormalTok{find_wealth_history_matrix(policies, state0, agent_shocks_matrices):}
    \KeywordTok{pass}
\end{Highlighting}
\end{Shaded}

\textbf{Helper functions:} There are a number of ``helper'' functions in
the SimulationLibrary which support
\emph{create\_income\_shocks\_experience} and
\emph{find\_wealth\_history\_matrix}. The single task of creating and
simulating the shocks is split across a number of helper functions to
aid in both clarity and unit testing. Together they are used to create
the final matrices of shocks needed to simulate consumption:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ------- Generate permanent and transitory income shocks ------- #}
\KeywordTok{def} \NormalTok{generate_permanent_income_draws(psi_sigma, N_simulate):}
    \KeywordTok{pass}

\KeywordTok{def} \NormalTok{generate_transitory_income_draws(temp_sigma, p_unemploy, N_simulate):}
    \KeywordTok{pass}

\CommentTok{# ------ Create joint discrete distribution from independent marginals ----- #}
\KeywordTok{def} \NormalTok{generate_all_combined_shocks(shocks1, p1, shocks2, p2):}
    \KeywordTok{pass}

\CommentTok{# ------ Generate matrix of perm and transitory shocks of correct size ------- #}
\KeywordTok{def} \NormalTok{create_shocks_matrix_1D(shocks, N_simulate, T, seed=}\OtherTok{None}\NormalTok{):}
    \KeywordTok{pass}

\KeywordTok{def} \NormalTok{create_shocks_matrix_1D_with_zero_income_event(shocks, N, T, seed=}\OtherTok{None}\NormalTok{):}
    \KeywordTok{pass}

\CommentTok{# ------ Generate retirement income if needed ------ #}
\KeywordTok{def} \NormalTok{generate_retire_income(psi_retire, xi_retire, Gamma, R, p_unemploy_retire):}
    \KeywordTok{pass}
\end{Highlighting}
\end{Shaded}

\subsubsection{Estimation.py}\label{estimation.py}

The final module in HACK is the Estimation module. This brings all the
others together to solve, simulate, and estimate the preference
parameters for the basic consumption-under-uncertainty problem outlined
above. In the pseudo-code below, all the major functions which are
necessary for the operation of the HACK project are outlined. To change
the baseline model, one only needs to change the five major functions
imported from other modules and used in the Estimation module. These
five functions and their definitions comprise the programming API for
the HACK framework:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  SimulationLibrary:

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    \emph{create\_income\_shocks\_experience}
  \item
    \emph{find\_wealth\_hist\_matrix}
  \end{itemize}
\item
  SolutionLibrary:

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    \emph{init\_consumer\_problem}
  \item
    \emph{solve\_consumption\_problem}
  \end{itemize}
\item
  SetupParameters:

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    \emph{find\_simulated\_medians}
  \end{itemize}
\end{itemize}

The module first imports all necessary HACK modules, executes the
primary functions from each, creates the SMM objective function
$\varpi_{\varrho}(\phi)$, and executes a single minimization:

\[\underset{\phi}{min} \; \varpi_{\varrho}(\phi).\]

Also included in this module is a bootstrap function which repeats the
minimization for a bootstrap sample of data, $N_{bootstrap}$ times.

A special note for the pseudo-code for Estimation.py that follows: the
Python operator ``**'' acts to unpack a dictionary (a hash-table data
storage object in Python) and use its key to associate the dictionary
values with the appropriate function calls. Thus the definitions of the
dictionaries ``unpacked'' by the ** symbols below act to keep the code
clean and readable.

This code excerpt includes the pseudo-code for the calculation of the
SMM objective function, \emph{smm\_objective\_fxn}, which is almost
entirely complete code:

\begin{Shaded}
\begin{Highlighting}[]
\CharTok{import} \NormalTok{SetupParameters }\CharTok{as} \NormalTok{param}
\CharTok{import} \NormalTok{SolutionLibrary }\CharTok{as} \NormalTok{solution}
\CharTok{import} \NormalTok{SimulationLibrary }\CharTok{as} \NormalTok{simulate}

\CommentTok{# ------ Create income shock draws ------ #}
\NormalTok{agent_shocks_matrices = simulate.create_income_shocks_experience(**param.create_income_shocks)}

\CommentTok{# ------ Initialize the consumer problem ------ #}
\NormalTok{income_distrib = solution.init_consumer_problem(**param.init_consumer_problem)}

\CommentTok{# ------ Create full collection of calibration parameters ------ #}
\NormalTok{calibrated_parameters = \{}\StringTok{"shocks"}\NormalTok{:agent_shocks_matrices, }\StringTok{"income"}\NormalTok{:income_distrib\}}
\NormalTok{calibrated_parameters.update(param.calibrated_parameters)}

\CommentTok{# ------ Define the SMM objective function ------ #}
\KeywordTok{def} \NormalTok{smm_objective_fxn(phi, **calibrated_parameters):}
    \CommentTok{#  Solve consumption functions}
    \NormalTok{consumption = solution.solve_consumption_problem(phi, ...)}
    \CommentTok{# Run simulation and get simulated wealth holdings}
    \NormalTok{sim_m_history = simulate.find_wealth_hist_matrix(...)}
    \CommentTok{# Construct simulated medians correctly for efficient calculation...}
    \NormalTok{simulated_medians = param.find_simulated_medians(sim_m_history)}
    \CommentTok{# Return sum of absolute errors}
    \KeywordTok{return} \NormalTok{np.dot(empirical_weights, np.}\DataTypeTok{abs}\NormalTok{(empirical_data - simulated_medians))}


\CommentTok{# ------ Execute a single minimization on the SMM objective function ------ #}
\KeywordTok{def} \NormalTok{minimize_smm_objective(minimizer):}
    \KeywordTok{pass}

\CommentTok{# ------ Bootstrap the minimization on the SMM objective function ------ #}
\KeywordTok{def} \NormalTok{bootstrap(optimizer, param.empirical_data, param.Ndraws, **calibrated_parameters):}
    \KeywordTok{pass}
\end{Highlighting}
\end{Shaded}

The beauty of the modular HACK structure emerges in this final
Estimation module. In all previous modules, extending the baseline model
may require much overwriting of the basic functions. However once that
writing is done, the work needed to extend the Estimation.py module is
minimal. In fact, the only changes likely necessary are to some of the
function inputs and outputs in the body of the
\emph{smm\_objective\_fxn} function.

\section{Summary and Conclusion}\label{summary-and-conclusion}

The HACK project is a modular code library for constructing
macroeconomic and macro-financial models with heterogeneous agents
solving portfolio decisions under uncertainty. Portfolio choice under
uncertainty is central to nearly all academic models, including modern
DSGE models (with and without financial sectors), models of asset
pricing (eg. CAPM and C-CAPM), models of financial frictions (eg.
Bernanke et al. 1999), and many more. Under the right assumptions many
of these models can be solved by aggregating agent decision-making and
employing the representative agent, with standardized computational
frameworks for solving these models. However when individual agents look
very different from one another - for example, different wealth levels,
preferences, or exposures to different types of shocks - assumptions
required for aggregation can quickly fail and a representative agent may
no longer be appropriate. Code to solve these models tends to be bespoke
and idiosyncratic, often reinvented by different researchers working on
similar problems. This needless code duplication increases the chance
for errors and wastes valuable researcher time.

Researchers should spend their valuable time producing research, not
reinventing the wheel when it comes to computational tools. The goal of
the HACK toolkit is to ease this burden by providing a simple and easily
extensible framework in which a few common models are solved, and clear
documentation, testing, and estimation frameworks provide guidance for
new researchers to develop their own work in a robust and replicable
manner. The final goals of the project are to create a collaborative
codebase which can serve both researchers and policymakers alike,
employing the best of modern software development tools to accelerate
understanding and implementation of cutting edge research tools. The
solution methods employed in HACK are not the only methods available,
and those who have additional methodological suggestions are strongly
encouraged to contribute! Increasing returns to production is one of the
few ``non-dismal'' possibilities in economic thought -- we hope to
capture this feature of code production in the HACK framework. Key next
steps include finalizing the general-equilibrium HACK modules,
identifying additional baseline models to replicate in HACK, and
encouraging a new generation of students to learn from, use, and
contribute to the collaborative construction of heterogeneous-agent
models.

\textbf{Bibliography}

Aruoba, S. B., and FernÃ¡ndez-Villaverde, J. (2014). ``A Comparison of
Programming Languages in Economics.'' No. W20263. National Bureau of
Economic Research.

Chacon, S. and Straub, B. (2014). Pro Git. Apress, 2014. At:
http://git-scm.com/book

Carroll, C. D. (2006). ``The method of endogenous gridpoints for solving
dynamic stochastic optimization problems.''" Economics letters, 91(3),
312-320.

Carroll, C. D. (2012a). ``Solving microeconomic dynamic stochastic
optimization problems.'' Mimeo. Johns Hopkins University Department of
Economics, 2012.

Carroll, C.D. (2012b). ``Implications of Wealth Heterogeneity For
Macroeconomics.'' Johns Hopkins University Department of Economics
Working Paper Number 597 May 2012.

Carroll, C.D. (2014a). ``Representing Consumption and Saving Without a
Representative Consumer.'' In Measuring Economic Sustainability and
Progress Studies in Income and Wealth. National Bureau of Economic
Research 2014.

Carroll, C.D. (2014b). ``Heterogeneous Agent Macroeconomics: An Example
and an Agenda.'' Presentation at IMF Workshop on Computational
Macroeconomics. Washington, December 2014.

Carroll, C.D., Slacalek, J., Tokuoka, K., and White, M.N. (2015). ``The
Distribution of Wealth and the Marginal Propensity to Consume.''" Draft
Johns Hopkins University March 2015.

Geanakoplos, J. (2009). ``The leverage cycle.''" In~NBER Macroeconomics
Annual 2009, Volume 24~(pp.~1-65). University of Chicago Press.

Geanakoplos, J., Axtell, R., Farmer, D. J., Howitt, P., Conlee, B.,
Goldstein, J., Hendry, M., Palmer, N. M., and Yang, C. Y. (2012).
``Getting at systemic risk via an agent-based model of the housing
market.''~The American Economic Review,~102(3), 53-58.

Ram, Y., and Hadany, L. (2015). ``The probability of improvement in
Fisher's geometric model: A probabilistic approach.''" Theoretical
population biology, 99, 1-6.

Sargent, T. and Stachurski, J. (2015). ``Quantitative Economics with
Python.'' Lecture Notes. Accessed 16 May 2015. At:
http://quant-econ.net/\_static/pdfs/py-quant-econ.pdf

Sheppard, K. (2014). ``Introduction to Python for Econometrics,
Statistics and Numerical Analysis: Second Edition.'' Lecture Notes.
Accessed 16 May 2015. At:
https://www.kevinsheppard.com/Python\_for\_Econometrics

\end{document}
